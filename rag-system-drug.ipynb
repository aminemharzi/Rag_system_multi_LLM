{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10401121,"sourceType":"datasetVersion","datasetId":6444752}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:36:37.374926Z","iopub.execute_input":"2025-01-11T17:36:37.375244Z","iopub.status.idle":"2025-01-11T17:36:44.953957Z","shell.execute_reply.started":"2025-01-11T17:36:37.375219Z","shell.execute_reply":"2025-01-11T17:36:44.952559Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install replicate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:36:44.955734Z","iopub.execute_input":"2025-01-11T17:36:44.956093Z","iopub.status.idle":"2025-01-11T17:36:49.093477Z","shell.execute_reply.started":"2025-01-11T17:36:44.956063Z","shell.execute_reply":"2025-01-11T17:36:49.092109Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: replicate in /usr/local/lib/python3.10/dist-packages (1.0.4)\nRequirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.1)\nRequirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.9.2)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.12.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.23.4)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.2)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:36:49.095801Z","iopub.execute_input":"2025-01-11T17:36:49.096160Z","iopub.status.idle":"2025-01-11T17:36:53.699947Z","shell.execute_reply.started":"2025-01-11T17:36:49.096131Z","shell.execute_reply":"2025-01-11T17:36:53.698255Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:36:53.701965Z","iopub.execute_input":"2025-01-11T17:36:53.702417Z","iopub.status.idle":"2025-01-11T17:37:00.350250Z","shell.execute_reply.started":"2025-01-11T17:36:53.702372Z","shell.execute_reply":"2025-01-11T17:37:00.348574Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3d7fd7f06dfea66e9605ca486694841929bb2908aacc628ba3ce0580533006ee\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport google.generativeai as genai\nimport uuid  # For generating UUIDs\n\n############################################\n# 1) Read data from CSV file\n############################################\nCSV_FILE_PATH = \"/kaggle/input/drug-label-dataset/sample.csv\"  # Replace with your actual CSV file path\n\n# Load the CSV file into a Pandas DataFrame\n# Assuming the CSV has a column named \"context\" with text data to process\ndf = pd.read_csv(CSV_FILE_PATH)\nprint(f\"Loaded {len(df)} rows from CSV file.\")\n\n############################################\n# 2) Process the DataFrame\n############################################\n# Create chunks of text from the \"context\" column\nrecords = []\nMAX_TOKENS = 3000  # Example token limit for chunking\n\nfor _, row in df.iterrows():\n    # Safely get the context field\n    context = str(row.get(\"context\", \"\")).strip()  # Ensure the column name matches your CSV\n    if context:\n        # Create chunks by token size if necessary\n        words = context.split()\n        chunks = [\" \".join(words)]  # Modify as needed to split into smaller chunks\n        for chunk in chunks:\n            records.append({\n                \"chunk\": chunk,\n                \"uuid\": str(uuid.uuid4())  # Assign a unique UUID to each chunk\n            })\n\n# Convert to a Pandas DataFrame\nprocessed_df = pd.DataFrame(records)\nprint(f\"Created {len(processed_df)} rows after chunking.\")\n\n# Replace NaN values with empty strings\nprocessed_df.fillna(\"\", inplace=True)\n\n############################################\n# 3) Save the processed data to CSV\n############################################\noutput_csv_path = \"processed_data.csv\"\nprocessed_df.to_csv(output_csv_path, index=False)\nprint(f\"Processed data saved to {output_csv_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:00.351836Z","iopub.execute_input":"2025-01-11T17:37:00.352283Z","iopub.status.idle":"2025-01-11T17:37:23.008259Z","shell.execute_reply.started":"2025-01-11T17:37:00.352249Z","shell.execute_reply":"2025-01-11T17:37:23.006884Z"}},"outputs":[{"name":"stdout","text":"Loaded 100 rows from CSV file.\nCreated 100 rows after chunking.\nProcessed data saved to processed_data.csv.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"processed_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:23.009622Z","iopub.execute_input":"2025-01-11T17:37:23.010031Z","iopub.status.idle":"2025-01-11T17:37:23.025257Z","shell.execute_reply.started":"2025-01-11T17:37:23.010000Z","shell.execute_reply":"2025-01-11T17:37:23.024144Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                               chunk  \\\n0  Brand Name: Bismuth Stibium, Generic Name: BIS...   \n1  Effective Time: 20240118, Effective Date: 2024...   \n2  Brand Name: Omeprazole, Sodium bicarbonate, Ge...   \n3  Brand Name: Diltiazem Hydrochloride Extended-R...   \n4  Brand Name: FLUDROCORTISONE ACETATE, Generic N...   \n\n                                   uuid  \n0  a1f0d44f-d419-413e-b797-00dac391d9f3  \n1  7f3ffb22-9de3-4da0-8e3f-a7bfd0559439  \n2  f77124f7-7b23-4f9d-a60d-ce0b537aca7b  \n3  948f7a14-2725-4f12-8159-c75761e677ec  \n4  e2921711-6b00-431f-bcbd-f2b7aa8252ed  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk</th>\n      <th>uuid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brand Name: Bismuth Stibium, Generic Name: BIS...</td>\n      <td>a1f0d44f-d419-413e-b797-00dac391d9f3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Effective Time: 20240118, Effective Date: 2024...</td>\n      <td>7f3ffb22-9de3-4da0-8e3f-a7bfd0559439</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brand Name: Omeprazole, Sodium bicarbonate, Ge...</td>\n      <td>f77124f7-7b23-4f9d-a60d-ce0b537aca7b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Brand Name: Diltiazem Hydrochloride Extended-R...</td>\n      <td>948f7a14-2725-4f12-8159-c75761e677ec</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Brand Name: FLUDROCORTISONE ACETATE, Generic N...</td>\n      <td>e2921711-6b00-431f-bcbd-f2b7aa8252ed</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"processed_df.iloc[0, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:23.026278Z","iopub.execute_input":"2025-01-11T17:37:23.026589Z","iopub.status.idle":"2025-01-11T17:37:23.052044Z","shell.execute_reply.started":"2025-01-11T17:37:23.026558Z","shell.execute_reply":"2025-01-11T17:37:23.050610Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'Brand Name: Bismuth Stibium, Generic Name: BISMUTH STIBIUM, Manufacturer Name: Uriel Pharmacy Inc., Product Type: HUMAN OTC DRUG, Route: TOPICAL, Effective Time: 20240122, Effective Date: 2024-01-22, Active Ingredient: Active Ingredients: 100 gm contains: 25 gm Allium sativa (Garlic) 1X, 25 gm Chelidonium (Greater celandine) 1X, 25 gm Curcuma (Turmeric) 1X, 25 gm Thuja (American arborvitae) 1X, 20 gm Bismuth 2X, 20 gm Stibium met. (Antimony) 2X, Warnings: Warnings: FOR EXTERNAL USE ONLY. Claims based on traditional homeopathic practice, not accepted medical evidence. Not FDA evaluated. Do not use if allergic to any ingredient. Consult a doctor before use for serious conditions, if conditions worsen or persist, or accidental ingestion occurs. If pregnant or nursing, consult a doctor before use. Avoid contact with eyes. Do not use if safety seal is broken or missing., Indications and Usage: Directions: FOR TOPICAL USE ONLY., Purpose: Use: Temporary relief of warts., Dosage and Administration: Apply once or twice daily to warts. Under age 2: Consult a doctor, Keep Out of Reach of Children: KEEP OUT OF REACH OF CHILDREN., Inactive Ingredient: Inactive Ingredients: White petrolatum, Lanolin, Mineral oil, Glyceryl monostearate, Sorbic acid, Tea tree oil, Grapefruit seed extract \"prepared using rhythmical processes\", Questions: Questions? Call 866.642.2858 Made with care by Uriel, East Troy, WI 53120 shopuriel.com Lot:, SPL Product Data Elements: Bismuth Stibium Bismuth Stibium PETROLATUM LANOLIN MINERAL OIL GLYCERYL MONOSTEARATE SORBIC ACID CITRUS PARADISI SEED GARLIC GARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS TURMERIC TURMERIC THUJA OCCIDENTALIS WHOLE THUJA OCCIDENTALIS WHOLE BISMUTH BISMUTH ANTIMONY ANTIMONY TEA TREE OIL, Package Label Principal Display Panel: Bismuth Stibium Ointment'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n# Load the SentenceTransformer model\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Generate embeddings for the context column\nprocessed_df['embeddings'] = processed_df['chunk'].apply(lambda x: model.encode(x).tolist())\n\n# Save embeddings for later use\nprocessed_df.to_parquet(\"data_with_embeddings_final.parquet\", index=False)\nprint(\"Data with embeddings saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:23.055363Z","iopub.execute_input":"2025-01-11T17:37:23.055698Z","iopub.status.idle":"2025-01-11T17:37:37.268124Z","shell.execute_reply.started":"2025-01-11T17:37:23.055667Z","shell.execute_reply":"2025-01-11T17:37:37.266856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c17b0c16a224618a07df0820a5cff12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd808e2873514a5a88774b67d1685940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dca4661699a4f5ea4ed176e718f1f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2923dbe75b43fab99680f078215f97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce67b2ab36b4b38ba91c2921bfba7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7da53c660bf45a880d40b25712355c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8e0f22f5124fd9a7aa85156806a148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8a46c18ba24b6a84c646be85597a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37df2217406a4a07b542d90c9c503fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d06dfb378645ab96f5735467ff5129"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2850adba7df422cb71c781e0ce6c6db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114497385e4e46348f7c0fa9a2952a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a38566cf76c4669bde60877019e569e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e1e0cb2c2ce4be68c385aa2e0079d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9f70d458d1464496a76f35e338c0c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd64e341e942484ab1943aab00a6f7d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dcad93a3bcf4787a3379d88f102a65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f91e4f7b0f0a4978b351ff091fab3be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c1749421a947da816ee0804e12814d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72518945e604a73ae220ebea625b82f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c326dbc58316457aaa3b3398b980e3ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efbb8ab49d364c498d63fc59bd141369"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5f6edb8c204a6ebbd1ac79649a7ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01f0658ebda4858a178b7eb880c3c19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87d46912e5d4d1eb840ba933257e63c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc62ba5e72a4f918ffdde5a05430756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfaa9dc2169f42b1b28abbf5f8c20a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6d58a5af86438a81e31701f51f0e1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a150dddced51422b87bb4c533a5a510f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf7a3403c0043948ad04a40be3fd8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a072af330966464ca49dc9669fa3b094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec43c93dbb154c7f8e26b481afcdc168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0deae81b608242c1ba709484c8c937aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9f36e23ff04c7aa3bee687504efb02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a1ddb2af5a464ba9eb87ffd7e8a5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098377a062a1442982ee48fcad051559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9dfd1b986f24b139916f40d52f4796d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb65056cb9846a28da52cdfc77c28e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be62d8874d1a4b2abcfd9e92a1f6505b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"810d973fbc7f41a885fb08fcc32c5f19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22219aa59ec04e1f8c30d92f9f2bf993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ae50c2389a4c219c1943df9764d78f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da54f61bb4be48488228d42fdc0478d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5b3847c6394553ac58cf14e19a603f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26635be759aa44c59eb892fde35e4bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5f8ea5c789405abe31f2a3f7f8027c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1c06d0f1e8446f83317c1064327648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6ed50ef182849d4a2fe009948d2f8a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5869cc24d24643a4a097e4602043af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a470c1508684e9aaa58e797d2b09ed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa012561b30411083b14e86a35527c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084d5be7debe495fa77731a83c4bcd6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6aab3927a34d7c857a697605818b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7500cb4d3ab04867be05cce59222501d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ed7db9cf3e4f949bb3afe5ffa58f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bf911a1d974ec59e7b89fc6bc87338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965ea44a13ca4442ae67a2811d46622b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32de1e82faac49738f0f9f5bae702464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef500c73a98a473088c2628b3c588865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b213a55ef86d4ad7bd1eec197a037f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"983477fda5964ce8bdec26eb66157c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2042764eefe4445a86c82986c584fd39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4abd096c7124d96a54cd2eba5658c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0427dcc374548b1b837b466943cd511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c545820666c4b41bd209d91272b46eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9a0ef646f048e7ba6325f221a09566"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ddd4790a8948f2b31d391822f23b27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f7ef1f1b61469384aa9b5455d700b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf6c67eaca44dbd82dbd714487c68f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4eab5b6cca34c07af6b4944ff36bf47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a66ad86b724ecdbf3fcf1c425668ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7621ac5001b648d596143f8d617feda3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1780c4e00e4f439d8b02af06361e5ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c1da67dda647deaa8b0da39309d2e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99bf8f5331045e1bdf86e79332bc94c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1950fa5ecd664098b70f95ec4f73c972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb35b6b71394511abd2d61173024ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322ee009a1f243f3bd4975f41ffb8861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce402af06ac846f1ab5ab9a7b819805f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fba5855190d43a7815fec68bff23cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555f7129544c4a18993e7bcb0b6fb695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4fdb697ded4c77aa44702c1529bb9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b310447a6940d4961fc8c5498ccdc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b379c70973b34d6c912adb22624f4523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739ec89957504c5394ba75698badaec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe4247b2a044c75a35dd72244af591d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07d32af952e4268a1151bdf093ee9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ec7da967124666bdbd81a6331fba41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccfaffc4f25c40f49290384d663bad62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3a254f67dd48e2892ff32d30b432b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83712830ed14c9eb55f90d437ee4ddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff65f1dee8440b8b99b66a2be981048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57a691112b5a410b93577133daa71fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e466e6dd594650b2ab2d7dbb8547cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61373d467ad34019af6104b2dc03fed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f23fb72ed44fb2b6a7d1b9010721bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bb88fd190c46cbaea59408954c8637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"899fff3c897e4e8590608a7d50d9f413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a068c47129cd4280a04d9ef9078bc36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da75b6bf25dd474dbf7a5d39fa09b511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3225e86307dd42a1a0f984aa9d66c691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a316a0763942d491efa18b98f6cd5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14131b2b67e2476ba1ddba5ee135c988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b2e8c7a21534f789e0ecdc0a44fbfa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef2781918e34e2c9450d96f06f1f583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8c08a5605284645828c3642381a6bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb53baf05af2432e80ad05807fd40a6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f64f1f5e70452c946307a5847f1f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0eea88c96d341f69bd6d78bc9bd0e69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4c29bc198e4b6caf78d15fe3bda23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897175a34e9e42c7bbe778f139865668"}},"metadata":{}},{"name":"stdout","text":"Data with embeddings saved successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import faiss\nimport numpy as np\n\n# Convert embeddings to a numpy array\nembeddings = np.array(processed_df['embeddings'].tolist(), dtype='float32')\n\n# Create a FAISS index\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings)\n\n# Save the FAISS index\nfaiss.write_index(index, \"faiss_index_final.bin\")\nprint(\"FAISS index saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.269984Z","iopub.execute_input":"2025-01-11T17:37:37.270404Z","iopub.status.idle":"2025-01-11T17:37:37.279476Z","shell.execute_reply.started":"2025-01-11T17:37:37.270369Z","shell.execute_reply":"2025-01-11T17:37:37.278274Z"}},"outputs":[{"name":"stdout","text":"FAISS index saved successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\ndef generate_answer_with_context_t5(query, context):\n    model_name = \"t5-large\"  # or \"t5-base\", \"t5-large\"\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n    \n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    outputs = model.generate(input_ids=inputs['input_ids'], max_length=100, num_beams=4, early_stopping=True)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.280904Z","iopub.execute_input":"2025-01-11T17:37:37.281267Z","iopub.status.idle":"2025-01-11T17:37:37.311730Z","shell.execute_reply.started":"2025-01-11T17:37:37.281235Z","shell.execute_reply":"2025-01-11T17:37:37.310602Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_answer_with_context_gpt2(query, context):\n    model_name = \"gpt2\"  # You can use \"gpt2-medium\", \"gpt2-large\" for larger models\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    \n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    \n    # Increase max_length to a higher value, ensuring it's within the model's token limit\n    max_input_length = 1024  # Ensure this is within the model's max token limit\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n    \n    # Set max_new_tokens to control the length of the generated answer\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=150, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Remove the prompt part from the output and return the answer\n    answer = generated_text[len(input_text):].strip()\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.313072Z","iopub.execute_input":"2025-01-11T17:37:37.313448Z","iopub.status.idle":"2025-01-11T17:37:37.354334Z","shell.execute_reply.started":"2025-01-11T17:37:37.313410Z","shell.execute_reply":"2025-01-11T17:37:37.353289Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n\n\ndef generate_answer_with_context_bert(query, context):\n    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n    model = BertForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n\n    # Prepare the input text for BERT\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Print the tokenized input to check if the context and question are properly encoded\n    # print(\"Tokenized Input:\", inputs)\n\n    # Run the model to get start and end logits\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n\n    # Print the logits to see what the model is predicting\n    # print(\"Start Scores:\", start_scores)\n    # print(\"End Scores:\", end_scores)\n\n    # Get the most likely start and end positions\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n\n    # Check if the answer span is valid\n    if answer_start >= answer_end:\n        print(\"No valid answer found.\")\n        return \"No valid answer found.\"\n\n    # Decode the answer (removes [CLS], [SEP] tokens and gives you the answer span)\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end], skip_special_tokens=True)\n    \n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.355469Z","iopub.execute_input":"2025-01-11T17:37:37.356011Z","iopub.status.idle":"2025-01-11T17:37:37.363172Z","shell.execute_reply.started":"2025-01-11T17:37:37.355976Z","shell.execute_reply":"2025-01-11T17:37:37.361893Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\nimport torch\n\ndef generate_answer_with_context_distilbert(query, context):\n    model_name = \"distilbert-base-uncased-distilled-squad\"\n    model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end])\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.364478Z","iopub.execute_input":"2025-01-11T17:37:37.364963Z","iopub.status.idle":"2025-01-11T17:37:37.400206Z","shell.execute_reply.started":"2025-01-11T17:37:37.364930Z","shell.execute_reply":"2025-01-11T17:37:37.398800Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import google.generativeai as genai\n\ndef generate_answer_with_context_gemini(query, context):\n    # Initialize Google Generative AI model\n    genai.configure(api_key=\"AIzaSyAMCnx5Wde22yIsRL53lB20FETVkCXe2Ws\")\n    # gemini_model = genai.GenerativeModel('gemini-pro')\n    gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n    \n    # Construct the prompt\n    prompt = f\"Given the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    try:\n        # Use the correct method to generate an answer, check if `generate_content` or `generate` is available\n        answer = gemini_model.generate_content(prompt)  # Correct method usage\n        # print(answer.text)\n\n        \n        # Print and return the answer\n        print(\"Generated Answer:\")\n        return answer.text\n        \n    except Exception as e:\n        print(f\"Error while generating content: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.401278Z","iopub.execute_input":"2025-01-11T17:37:37.401674Z","iopub.status.idle":"2025-01-11T17:37:37.409191Z","shell.execute_reply.started":"2025-01-11T17:37:37.401631Z","shell.execute_reply":"2025-01-11T17:37:37.407974Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import BloomForCausalLM, BloomTokenizerFast\n\ndef generate_answer_with_context_bloom(query, context):\n    model_name = \"bigscience/bloom-560m\"\n    model = BloomForCausalLM.from_pretrained(model_name)\n    tokenizer = BloomTokenizerFast.from_pretrained(model_name)\n\n    # Create the prompt\n    input_text = f\"Given the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    # Tokenize input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n\n    # Generate the response\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=100)  # Control only new token generation\n\n    # Decode the generated tokens to text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = generated_text.split('Question: ')[-1]\n\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.410355Z","iopub.execute_input":"2025-01-11T17:37:37.410792Z","iopub.status.idle":"2025-01-11T17:37:37.438484Z","shell.execute_reply.started":"2025-01-11T17:37:37.410736Z","shell.execute_reply":"2025-01-11T17:37:37.437217Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_answer_with_context_distilgpt2(query, context):\n    model_name = \"distilgpt2\"\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=150, temperature=0.9)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = generated_text.split('Question: ')[-1]\n\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.439717Z","iopub.execute_input":"2025-01-11T17:37:37.440186Z","iopub.status.idle":"2025-01-11T17:37:37.450887Z","shell.execute_reply.started":"2025-01-11T17:37:37.440138Z","shell.execute_reply":"2025-01-11T17:37:37.449736Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from transformers import ElectraForQuestionAnswering, ElectraTokenizer\nimport torch\n\ndef generate_answer_with_context_electra(query, context):\n    model_name = \"google/electra-small-discriminator\"\n    model = ElectraForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = ElectraTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end])\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.452029Z","iopub.execute_input":"2025-01-11T17:37:37.452419Z","iopub.status.idle":"2025-01-11T17:37:37.481195Z","shell.execute_reply.started":"2025-01-11T17:37:37.452389Z","shell.execute_reply":"2025-01-11T17:37:37.479932Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from transformers import ReformerForSequenceClassification, ReformerTokenizer\n\ndef generate_answer_with_context_reformer(query, context):\n    model_name = \"google/reformer-enwik8\"\n    model = ReformerForSequenceClassification.from_pretrained(model_name)\n    tokenizer = ReformerTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n    outputs = model.generate(input_ids=inputs['input_ids'], max_length=150)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.482339Z","iopub.execute_input":"2025-01-11T17:37:37.482635Z","iopub.status.idle":"2025-01-11T17:37:37.501538Z","shell.execute_reply.started":"2025-01-11T17:37:37.482607Z","shell.execute_reply":"2025-01-11T17:37:37.500258Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"r8_SXDsjova2x9iSRWWk7nFiLCdExvApO725gF6l\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:37:37.502894Z","iopub.execute_input":"2025-01-11T17:37:37.503366Z","iopub.status.idle":"2025-01-11T17:37:37.508309Z","shell.execute_reply.started":"2025-01-11T17:37:37.503318Z","shell.execute_reply":"2025-01-11T17:37:37.507168Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import replicate\n\n\n\ndef generate_answer_with_context_LAMA_2(query, context):\n   \n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    input = {\n    \"top_p\": 1,\n    \"prompt\": input_text,\n    \"temperature\": 0.75,\n    \"max_new_tokens\": 800\n    }\n    \n    generated_text= \"\"\n    for event in replicate.stream(\n        \"meta/llama-2-7b-chat\",\n        input=input\n    ):\n        # print(event, end=\"\")\n        generated_text += str(event)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:45:06.170376Z","iopub.execute_input":"2025-01-11T17:45:06.170769Z","iopub.status.idle":"2025-01-11T17:45:06.177435Z","shell.execute_reply.started":"2025-01-11T17:45:06.170735Z","shell.execute_reply":"2025-01-11T17:45:06.175584Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# import replicate\n\n# input = {\n#     \"top_p\": 1,\n#     \"prompt\": \"Tell me how to tailor a men's suit so I look fashionable.\",\n#     \"temperature\": 0.75,\n#     \"max_new_tokens\": 800\n# }\n\n# for event in replicate.stream(\n#     \"meta/llama-2-7b-chat\",\n#     input=input\n# ):\n#     print(event, end=\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:38:38.846966Z","iopub.execute_input":"2025-01-11T17:38:38.847342Z","iopub.status.idle":"2025-01-11T17:38:38.852036Z","shell.execute_reply.started":"2025-01-11T17:38:38.847306Z","shell.execute_reply":"2025-01-11T17:38:38.850861Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\n# Load the FAISS index\nindex = faiss.read_index(\"faiss_index_final.bin\")\n\n# Example query\nquery = \"what is Bismuth Stibium ?\"\nquery_embedding = model.encode(query).astype('float32').reshape(1, -1)\n\n# Retrieve the top 5 most relevant contexts\ndistances, indices = index.search(query_embedding, 5)\n\n# Extract the relevant contexts\nrelevant_contexts = processed_df.iloc[indices[0]]['chunk'].tolist()\n\n# print('relevent context : ',relevant_contexts)\ncontext = relevant_contexts[0]\n\n# For GPT-2\ngenerated_answer = generate_answer_with_context_gpt2(query, context)\nprint(\"GPT-2 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For T5\ngenerated_answer = generate_answer_with_context_t5(query, context)\nprint(\"T5 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For BLOOM\ngenerated_answer = generate_answer_with_context_bloom(query, context)\nprint(\"bloom model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n\n# For GEMINI\ngenerated_answer = generate_answer_with_context_gemini(query, context)\nprint(\"GEMINI model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For GEMINI\n# generated_answer = generate_answer_with_context_reformer(query, context)\n# print(\"REFORMER model\")\n# print(generated_answer)\n# print(\"------------------------\")\n\n# For Electra\ngenerated_answer = generate_answer_with_context_electra(query, context)\nprint(\"Electra model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For Distilgpt2\ngenerated_answer = generate_answer_with_context_distilgpt2(query, context)\nprint(\"Distilgpt2 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n\n# For LLAMA 2\ngenerated_answer = generate_answer_with_context_LAMA_2(query, context)\nprint(\"LAMA-2 model\")\nprint(generated_answer)\nprint(\"------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:20:27.814026Z","iopub.execute_input":"2025-01-11T18:20:27.814570Z","iopub.status.idle":"2025-01-11T18:22:07.172784Z","shell.execute_reply.started":"2025-01-11T18:20:27.814528Z","shell.execute_reply":"2025-01-11T18:22:07.171336Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7395a3970a642cc8327a05bc777c0fa"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"GPT-2 model\nIt is a natural compound that is used to treat wart, ulcers, and other conditions. It has been used for centuries to relieve warty, sore throats, to prevent the spread of diseases, as well as to cure the common cold.\nBismutum St.ibum is an alkaloid found in the bark of the tree of B. stibus. The bark is composed of a mixture of two parts: the alkaline and the insoluble part. Bistum st.bismum contains a compound called bismulose, which is found naturally in bark. This compound is known as bistulosulfate. In the United States, it is also known by the name bicarbonate\n------------------------\nT5 model\nGARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJ\n------------------------\nbloom model\nwhat is Bismuth Stibium ?\nAnswer: Bismuth Stibium is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived\n------------------------\nGenerated Answer:\nGEMINI model\nBismuth Stibium is a homeopathic ointment marketed for the temporary relief of warts.  It contains Bismuth and Stibium (Antimony) along with several other homeopathic ingredients like garlic, greater celandine, turmeric, and American arborvitae.  It is for topical use only and is not FDA evaluated.\n\n------------------------\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Electra model\n: bismuth stibium, generic name : bismut\n------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Distilgpt2 model\nwhat is Bismuth Stibium?\nAnswer: Bismuth Stibium is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is\n------------------------\nLAMA-2 model\n Hello! I'm here to help you with your question. Bismuth Stibium is a topical ointment that contains various homeopathic ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, Stibium met. (Antimony), and others. It is manufactured by Uriel Pharmacy Inc. and is designed to provide temporary relief from warts.\n\nHowever, I must inform you that the safety and effectiveness of Bismuth Stibium have not been evaluated by the FDA, and its claims are based on traditional homeopathic practice rather than accepted medical evidence. As such, it is important to consult with a doctor before using this product, especially if you have a serious medical condition or accidental ingestion occurs.\n\nAdditionally, it is important to follow the instructions for use carefully and avoid contact with the eyes. If you have any questions or concerns, please call 866.642.2858 for more information.\n\nIn summary, Bismuth Stibium is a topical ointment that contains various homeopathic ingredients and is designed to provide temporary relief from warts. However, its safety and effectiveness have not been evaluated by the FDA, and it is important to consult with a doctor before using it.\n------------------------\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import numpy as np\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport time\nimport torch\nimport pandas as pd\nimport nltk\nnltk.download('punkt')  # Required for BLEU score\n\n# First, let's define our test dataset with some sample questions and reference answers\ntest_data = [\n    {\n        'question': 'What is Bismuth Stibium?',\n        'context': context,  # Your existing context\n        'reference_answer': 'Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.'\n    },\n    # Add more test cases if you have them\n]\n\n# Keep your existing model functions\nmodels = {\n    'GPT-2': generate_answer_with_context_gpt2,\n    'T5': generate_answer_with_context_t5,\n    # 'BERT': generate_answer_with_context_bert,\n    # 'DistilBERT': generate_answer_with_context_distilbert,\n    'BLOOM': generate_answer_with_context_bloom,\n    'LLAMA-2': generate_answer_with_context_LAMA_2\n}\n\ndef evaluate_single_answer(predicted, reference):\n    # Initialize ROUGE scorer\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    \n    # ROUGE scores\n    rouge_scores = scorer.score(predicted, reference)\n    \n    # BLEU score\n    ref_tokens = [reference.split()]\n    pred_tokens = predicted.split()\n    try:\n        bleu = sentence_bleu(ref_tokens, pred_tokens)\n    except:\n        bleu = 0\n    \n    # Exact Match\n    exact_match = 1 if predicted.lower().strip() == reference.lower().strip() else 0\n    \n    # F1 Score\n    pred_words = set(predicted.lower().split())\n    ref_words = set(reference.lower().split())\n    \n    precision = len(pred_words & ref_words) / len(pred_words) if pred_words else 0\n    recall = len(pred_words & ref_words) / len(ref_words) if ref_words else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n    \n    return {\n        'rouge1': rouge_scores['rouge1'].fmeasure,\n        'rouge2': rouge_scores['rouge2'].fmeasure,\n        'rougeL': rouge_scores['rougeL'].fmeasure,\n        'bleu': bleu,\n        'exact_match': exact_match,\n        'f1': f1\n    }\n\ndef evaluate_model(model_name, model_func, test_data):\n    all_metrics = []\n    execution_times = []\n    \n    for test_case in test_data:\n        # Measure execution time\n        start_time = time.time()\n        predicted_answer = model_func(test_case['question'], test_case['context'])\n        execution_time = time.time() - start_time\n        \n        # Calculate metrics\n        metrics = evaluate_single_answer(predicted_answer, test_case['reference_answer'])\n        metrics['execution_time'] = execution_time\n        all_metrics.append(metrics)\n        \n        # Print individual results\n        print(f\"\\n{model_name} Results for Question: {test_case['question']}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Reference Answer: {test_case['reference_answer']}\")\n        print(f\"Metrics: {metrics}\")\n    \n    # Calculate averages\n    avg_metrics = {\n        metric: np.mean([m[metric] for m in all_metrics])\n        for metric in all_metrics[0].keys()\n    }\n    \n    return avg_metrics\n\n# Run evaluation for all models\nresults = {}\nfor model_name, model_func in models.items():\n    print(f\"\\nEvaluating {model_name}...\")\n    try:\n        results[model_name] = evaluate_model(model_name, model_func, test_data)\n    except Exception as e:\n        print(f\"Error evaluating {model_name}: {str(e)}\")\n        continue\n\n# Create final report\ndf_results = pd.DataFrame(results).round(4)\nprint(\"\\nFinal Evaluation Results:\")\nprint(df_results)\n\n# Save results to CSV\ndf_results.to_csv('rag_evaluation_results.csv')\nprint(\"\\nResults saved to 'rag_evaluation_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:45:38.138693Z","iopub.execute_input":"2025-01-11T17:45:38.139195Z","iopub.status.idle":"2025-01-11T17:47:18.511793Z","shell.execute_reply.started":"2025-01-11T17:45:38.139156Z","shell.execute_reply":"2025-01-11T17:47:18.510578Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\nEvaluating GPT-2...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nGPT-2 Results for Question: What is Bismuth Stibium?\nPredicted Answer: This product is a topical product that is used to treat wart, ulcers, and other skin conditions. It is also used for topical use to prevent or treat other conditions such as acne, psoriasis, eczema, rashes, dry skin, skin cancer, etc.\nWhat is the difference between Bistuth and Bactulose? BISTUTH is an antiseptic that has been used in the treatment of psoriatic ulcerative colitis, acne vulgaris, dermatitis and psorsitis. BACTULOSE is another topical antisera that was used as a treatment for psorectal ulicosis. The difference is that BIS is not a preservative. This is because B\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.28723404255319157, 'rouge2': 0.053763440860215055, 'rougeL': 0.1595744680851064, 'bleu': 0.32030896835479866, 'exact_match': 0, 'f1': 0.20437956204379565, 'execution_time': 11.196508884429932}\n\nEvaluating T5...\n\nT5 Results for Question: What is Bismuth Stibium?\nPredicted Answer: GARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJ\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.03418803418803419, 'rouge2': 0.017391304347826087, 'rougeL': 0.03418803418803419, 'bleu': 0, 'exact_match': 0, 'f1': 0.025641025641025644, 'execution_time': 42.285906076431274}\n\nEvaluating BERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c969c1e9164c1d9afe92eaed27abfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbbcdf207a34abdabcb9f065989d50f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15107fef43064fe5a2b123993cd7956c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20a9218b5004785a0c7944dd57cfe40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0491a5da08df4ecd910753e852e654a9"}},"metadata":{}},{"name":"stdout","text":"\nBERT Results for Question: What is Bismuth Stibium?\nPredicted Answer: \nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0, 'bleu': 0, 'exact_match': 0, 'f1': 0, 'execution_time': 12.817095756530762}\n\nEvaluating DistilBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27ed7bc5af54b38bf82481b1d6d25bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c55fd9c2ed4fefa13c403e903a1e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a204632f2e8e4ff2a4843ba08fb3c0a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bea34f5e6a40e3809a9e5d8fa0ee8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e4d6eb9a10240549a532c57ff5a8810"}},"metadata":{}},{"name":"stdout","text":"\nDistilBERT Results for Question: What is Bismuth Stibium?\nPredicted Answer: \nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0, 'bleu': 0, 'exact_match': 0, 'f1': 0, 'execution_time': 4.07785701751709}\n\nEvaluating BLOOM...\n\nBLOOM Results for Question: What is Bismuth Stibium?\nPredicted Answer: What is Bismuth Stibium?\nAnswer: Bismuth Stibium is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.24175824175824176, 'rouge2': 0.044444444444444446, 'rougeL': 0.2087912087912088, 'bleu': 0.035371965931008, 'exact_match': 0, 'f1': 0.17582417582417584, 'execution_time': 26.47500443458557}\n\nEvaluating LLAMA-2...\n\nLLAMA-2 Results for Question: What is Bismuth Stibium?\nPredicted Answer:  Hello! I'm here to help you with your question. Bismuth Stibium is a topical ointment that is used to treat warts. It contains several homeopathic ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium met. (Antimony). These ingredients are said to have antiviral and ant-inflammatory properties, which may help to reduce the size and number of warts on the skin.\n\nIt's important to note that while Bismuth Stibium may be effective in treating warts, it is not a cure-all and may not work for everyone. It's also important to follow the instructions for use carefully and to consult a doctor before use if you have any concerns or if your symptoms persist or worsen.\n\nIn terms of safety, Bismuth Stibium is generally considered safe to use when used as directed. However, it's important to avoid using it near the eyes or on broken skin, and to consult a doctor before use if you are pregnant or nursing.\n\nI hope this information is helpful! Let me know if you have any other questions.\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.4115523465703971, 'rouge2': 0.1890909090909091, 'rougeL': 0.28158844765342966, 'bleu': 0.14146354640523665, 'exact_match': 0, 'f1': 0.425531914893617, 'execution_time': 2.4321658611297607}\n\nFinal Evaluation Results:\n                  GPT-2       T5     BERT  DistilBERT    BLOOM  LLAMA-2\nrouge1           0.2872   0.0342   0.0000      0.0000   0.2418   0.4116\nrouge2           0.0538   0.0174   0.0000      0.0000   0.0444   0.1891\nrougeL           0.1596   0.0342   0.0000      0.0000   0.2088   0.2816\nbleu             0.3203   0.0000   0.0000      0.0000   0.0354   0.1415\nexact_match      0.0000   0.0000   0.0000      0.0000   0.0000   0.0000\nf1               0.2044   0.0256   0.0000      0.0000   0.1758   0.4255\nexecution_time  11.1965  42.2859  12.8171      4.0779  26.4750   2.4322\n\nResults saved to 'rag_evaluation_results.csv'\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}