{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10401121,"sourceType":"datasetVersion","datasetId":6444752}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:48:18.253351Z","iopub.execute_input":"2025-01-10T18:48:18.253717Z","iopub.status.idle":"2025-01-10T18:48:30.015289Z","shell.execute_reply.started":"2025-01-10T18:48:18.253673Z","shell.execute_reply":"2025-01-10T18:48:30.013720Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:48:30.021298Z","iopub.execute_input":"2025-01-10T18:48:30.021660Z","iopub.status.idle":"2025-01-10T18:48:35.226375Z","shell.execute_reply.started":"2025-01-10T18:48:30.021629Z","shell.execute_reply":"2025-01-10T18:48:35.224841Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:48:35.227689Z","iopub.execute_input":"2025-01-10T18:48:35.228014Z","iopub.status.idle":"2025-01-10T18:48:41.970883Z","shell.execute_reply.started":"2025-01-10T18:48:35.227987Z","shell.execute_reply":"2025-01-10T18:48:41.969484Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e92b3ad7c62301ae3c2924e6ffc1bd32dffd6f85883c9e0b0ee659f47a3be74b\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport google.generativeai as genai\nimport uuid  # For generating UUIDs\n\n############################################\n# 1) Read data from CSV file\n############################################\nCSV_FILE_PATH = \"/kaggle/input/drug-label-dataset/sample.csv\"  # Replace with your actual CSV file path\n\n# Load the CSV file into a Pandas DataFrame\n# Assuming the CSV has a column named \"context\" with text data to process\ndf = pd.read_csv(CSV_FILE_PATH)\nprint(f\"Loaded {len(df)} rows from CSV file.\")\n\n############################################\n# 2) Process the DataFrame\n############################################\n# Create chunks of text from the \"context\" column\nrecords = []\nMAX_TOKENS = 3000  # Example token limit for chunking\n\nfor _, row in df.iterrows():\n    # Safely get the context field\n    context = str(row.get(\"context\", \"\")).strip()  # Ensure the column name matches your CSV\n    if context:\n        # Create chunks by token size if necessary\n        words = context.split()\n        chunks = [\" \".join(words)]  # Modify as needed to split into smaller chunks\n        for chunk in chunks:\n            records.append({\n                \"chunk\": chunk,\n                \"uuid\": str(uuid.uuid4())  # Assign a unique UUID to each chunk\n            })\n\n# Convert to a Pandas DataFrame\nprocessed_df = pd.DataFrame(records)\nprint(f\"Created {len(processed_df)} rows after chunking.\")\n\n# Replace NaN values with empty strings\nprocessed_df.fillna(\"\", inplace=True)\n\n############################################\n# 3) Save the processed data to CSV\n############################################\noutput_csv_path = \"processed_data.csv\"\nprocessed_df.to_csv(output_csv_path, index=False)\nprint(f\"Processed data saved to {output_csv_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:48:41.973473Z","iopub.execute_input":"2025-01-10T18:48:41.973786Z","iopub.status.idle":"2025-01-10T18:49:04.060913Z","shell.execute_reply.started":"2025-01-10T18:48:41.973761Z","shell.execute_reply":"2025-01-10T18:49:04.059647Z"}},"outputs":[{"name":"stdout","text":"Loaded 100 rows from CSV file.\nCreated 100 rows after chunking.\nProcessed data saved to processed_data.csv.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"processed_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:04.062923Z","iopub.execute_input":"2025-01-10T18:49:04.063326Z","iopub.status.idle":"2025-01-10T18:49:04.079037Z","shell.execute_reply.started":"2025-01-10T18:49:04.063280Z","shell.execute_reply":"2025-01-10T18:49:04.077850Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                               chunk  \\\n0  Brand Name: Bismuth Stibium, Generic Name: BIS...   \n1  Effective Time: 20240118, Effective Date: 2024...   \n2  Brand Name: Omeprazole, Sodium bicarbonate, Ge...   \n3  Brand Name: Diltiazem Hydrochloride Extended-R...   \n4  Brand Name: FLUDROCORTISONE ACETATE, Generic N...   \n\n                                   uuid  \n0  1837832a-eab3-4bec-b80f-5cc423a459fe  \n1  3be2d2a2-2a4c-40df-84b1-4a90efaef8f0  \n2  57cb304c-2c3e-41d0-9e53-3bbd76261e05  \n3  19711f90-3230-4018-a23b-9d6ce523066e  \n4  1263e60b-03a1-435f-b043-7a01cd91d1a0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chunk</th>\n      <th>uuid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brand Name: Bismuth Stibium, Generic Name: BIS...</td>\n      <td>1837832a-eab3-4bec-b80f-5cc423a459fe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Effective Time: 20240118, Effective Date: 2024...</td>\n      <td>3be2d2a2-2a4c-40df-84b1-4a90efaef8f0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brand Name: Omeprazole, Sodium bicarbonate, Ge...</td>\n      <td>57cb304c-2c3e-41d0-9e53-3bbd76261e05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Brand Name: Diltiazem Hydrochloride Extended-R...</td>\n      <td>19711f90-3230-4018-a23b-9d6ce523066e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Brand Name: FLUDROCORTISONE ACETATE, Generic N...</td>\n      <td>1263e60b-03a1-435f-b043-7a01cd91d1a0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"processed_df.iloc[0, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:04.079957Z","iopub.execute_input":"2025-01-10T18:49:04.080315Z","iopub.status.idle":"2025-01-10T18:49:04.652802Z","shell.execute_reply.started":"2025-01-10T18:49:04.080285Z","shell.execute_reply":"2025-01-10T18:49:04.651590Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'Brand Name: Bismuth Stibium, Generic Name: BISMUTH STIBIUM, Manufacturer Name: Uriel Pharmacy Inc., Product Type: HUMAN OTC DRUG, Route: TOPICAL, Effective Time: 20240122, Effective Date: 2024-01-22, Active Ingredient: Active Ingredients: 100 gm contains: 25 gm Allium sativa (Garlic) 1X, 25 gm Chelidonium (Greater celandine) 1X, 25 gm Curcuma (Turmeric) 1X, 25 gm Thuja (American arborvitae) 1X, 20 gm Bismuth 2X, 20 gm Stibium met. (Antimony) 2X, Warnings: Warnings: FOR EXTERNAL USE ONLY. Claims based on traditional homeopathic practice, not accepted medical evidence. Not FDA evaluated. Do not use if allergic to any ingredient. Consult a doctor before use for serious conditions, if conditions worsen or persist, or accidental ingestion occurs. If pregnant or nursing, consult a doctor before use. Avoid contact with eyes. Do not use if safety seal is broken or missing., Indications and Usage: Directions: FOR TOPICAL USE ONLY., Purpose: Use: Temporary relief of warts., Dosage and Administration: Apply once or twice daily to warts. Under age 2: Consult a doctor, Keep Out of Reach of Children: KEEP OUT OF REACH OF CHILDREN., Inactive Ingredient: Inactive Ingredients: White petrolatum, Lanolin, Mineral oil, Glyceryl monostearate, Sorbic acid, Tea tree oil, Grapefruit seed extract \"prepared using rhythmical processes\", Questions: Questions? Call 866.642.2858 Made with care by Uriel, East Troy, WI 53120 shopuriel.com Lot:, SPL Product Data Elements: Bismuth Stibium Bismuth Stibium PETROLATUM LANOLIN MINERAL OIL GLYCERYL MONOSTEARATE SORBIC ACID CITRUS PARADISI SEED GARLIC GARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS TURMERIC TURMERIC THUJA OCCIDENTALIS WHOLE THUJA OCCIDENTALIS WHOLE BISMUTH BISMUTH ANTIMONY ANTIMONY TEA TREE OIL, Package Label Principal Display Panel: Bismuth Stibium Ointment'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n# Load the SentenceTransformer model\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Generate embeddings for the context column\nprocessed_df['embeddings'] = processed_df['chunk'].apply(lambda x: model.encode(x).tolist())\n\n# Save embeddings for later use\nprocessed_df.to_parquet(\"data_with_embeddings_final.parquet\", index=False)\nprint(\"Data with embeddings saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:04.654184Z","iopub.execute_input":"2025-01-10T18:49:04.654650Z","iopub.status.idle":"2025-01-10T18:49:19.115130Z","shell.execute_reply.started":"2025-01-10T18:49:04.654598Z","shell.execute_reply":"2025-01-10T18:49:19.113947Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0795ee03ab4ebf9a2d15f1a78b3026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c6d55a5bf74e8f9c3cd611937dee29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996858d69059419391c57612f320b7ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2679ee7088a949d6aab28f56edefb308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a459b4459f43cf8ab92b5e05eaf007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526a03adc7534f7d86e88fd910cfb185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e11c017428422a97ebbddaec599744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb766944d0c94317b48b34840c3767c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bdca5d114604bd1afdf55c03580f5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac5988d03e444ec8dbaeec8a852da12"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f73fda43cb458183e1b6ee8e6ede27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f34d4b6780e244139106472d6d85489c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8eef9e9743e4284bc5fe2bfc0aa1998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b29c8a558e104f37a3f8be318b486a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb1cbf3424b4cb8808ba6c83847d4aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade664932a5f4ccd9771e02f1570e5c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd229f9f6eb0419d975f6b105ef5251f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6898032ed00148af9195d4208eec1ef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8f081fea4d411990c1bba9ba373cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b53c562fd94eb3a1c2df9fa03fbf1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071a588cff584649b4a2662bb86e686e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc920db066a4d0e9a0552ffa22bf5a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eed3ce77b26143cea6b70e3c5414fefa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b8e34a8f694f839a62863cb5b53a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7b946204814f93bbad5774e1c4de98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dde28a6208345d4acc1985709777547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b160a0ddba4d4245849b9ada88a5ba92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09417294dd85408486721930cb2ed712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276fdd2b38d0482d88e7c35298e543ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45c8751c36d4c7e8acea995bafdfc46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7369f919fdd443059cb2c89b9b911c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3b1dc5e66740189beabc5519a1b8a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1eee82249f45b78764bd9e834b1780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61fa95b2f574c48a681a52f6d2e91f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47680046c97441758b4a39ce198773ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8179cce1c1bc40458757cc7593db8b82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62257b6033541b5bc5249c26fbd546a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"586d6de8277748a0b4386bda7382eef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc309ac07e9c4a70b7cac7d04f837183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f424c00ade4b99ad0513a68e3bded8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e715105a3ac141a5a758f722a8a05e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a6759f21754744b5c58420390958cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d9b4213b7e46afb22aacf7ee756354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fde0010f0ba426abdeab914b8d97136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373a4ea873df4164b560b2b73cfa6cfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"191b5aac87ab41a6af731f611e959d9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff573b1c5051492883aa803b95a3de18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19fe8afa51b4f1492bf5b52ff40a038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d921030d11648948c9946cf44afad50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7db66d81f504dfaa5542958e2f91e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7717cf4035c4242856642a55d6ef295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d569a5bc32e418d89e682b71145c45e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96846817ce9495498d0d70995f36695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25be8c68b9d403e97723def0a03c877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d64439c04ae412aac3876660de0a224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe56af41e7ed430fa59e44f8466e1e50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732d0603e4a24c93a508c84140586069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b23a961d8af487bbb8e42e143e9b24f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"589460d1336e4c09bfa48c1d2156fc69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac27787775d4921a9586fcdb0c40261"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696476ecfa6942c8bbc295568b0a5803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bdea91c8244eefbd9f9dacb22ac4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451c338be40c4d3f9c791ae31353b9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f8d0123a014e9e8c850c54b7fab246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0abec6dd67f140e59756de594f0429eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dec4a3ff2864f349b370243d5bc96be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e1fe58a0dc4eeabdbbebe7a94a1187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63192d6ebad4e3496511821b629a45e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008b604d86f5446986fcdeacb061d3b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8cbe2f3487d4d36bb04af0fa9817a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ed3239dcb248e69ec4ee2c363b7e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20cf05f1660445e3931ae568ee078bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dccf467b744c4b98a8fb4e5e3ededa53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e748531b5c1448b82c96a64459669c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f990b745cb174664aa1a6f35e7352972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30829839b814afc9e4a50134dd0284c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3135de3d01de478f9f29edbc27193c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b598207e2224ef997671356ae089b44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1359af9b502d496ca6066c20b94a4c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65cd5067e3f47359f6d5d9f3aa58c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d27216d4d742d38b575495862fec1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e836c206f9642ef877cefd45aae5ff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fedcf20ab57e45919257c03f21244faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd97073571c4474183c79a3727b6dcbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd739d9821d144229fae38e2e5391b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f50dea785e41a098fa19d97742008d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b4f4951647492bb944210dab5220f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a36a7c07b3ab40289aea5fd10a4bfe52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf960b10ad04d8a935fdfa3a9eb8c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08fb639ba89b4561b2639e1b3941f575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268bfc4d5f9543f9855ec4db941ebe51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9dd6a65bc24983bece40e60416f060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ccb3a248f84451b8f5c2a8a8c466c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b805cec6b98d442ba145a22f3c842934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c807e334db4dc4979096601de444c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70feafe559c2417b9b56c1a5b9b93074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe0efbec00d4be09ef13fab66812ab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a98872fe4ed47f09a437a8a740f7145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be105b08565f439ab0699bcb65c1bafe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f528f4e1a2244954a657496cb9257717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4795d5804147908d7c17450c58b3e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a67392419f43da95f489835f8e4561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd359f05de14cd18c2175c03c0116ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ffe2736c17429a9d244f06588684dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763be04daf2040acaa4e65b464fbbe5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b54b6ecc894afe93ebebe4ade7fd34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28afd06320a8445f828f830854227aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3cb0d282da410298d525ff383ecd86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35dedf610094425e8e616f7c0d41e339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1087d6278407412691334805a6de2667"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f8d6175945470ba7845cd096e534f1"}},"metadata":{}},{"name":"stdout","text":"Data with embeddings saved successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import faiss\nimport numpy as np\n\n# Convert embeddings to a numpy array\nembeddings = np.array(processed_df['embeddings'].tolist(), dtype='float32')\n\n# Create a FAISS index\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings)\n\n# Save the FAISS index\nfaiss.write_index(index, \"faiss_index_final.bin\")\nprint(\"FAISS index saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.116233Z","iopub.execute_input":"2025-01-10T18:49:19.116754Z","iopub.status.idle":"2025-01-10T18:49:19.126370Z","shell.execute_reply.started":"2025-01-10T18:49:19.116706Z","shell.execute_reply":"2025-01-10T18:49:19.125250Z"}},"outputs":[{"name":"stdout","text":"FAISS index saved successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\ndef generate_answer_with_context_t5(query, context):\n    model_name = \"t5-large\"  # or \"t5-base\", \"t5-large\"\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n    \n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    outputs = model.generate(input_ids=inputs['input_ids'], max_length=100, num_beams=4, early_stopping=True)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.127712Z","iopub.execute_input":"2025-01-10T18:49:19.128182Z","iopub.status.idle":"2025-01-10T18:49:19.157119Z","shell.execute_reply.started":"2025-01-10T18:49:19.128141Z","shell.execute_reply":"2025-01-10T18:49:19.156047Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_answer_with_context_gpt2(query, context):\n    model_name = \"gpt2\"  # You can use \"gpt2-medium\", \"gpt2-large\" for larger models\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    \n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    \n    # Increase max_length to a higher value, ensuring it's within the model's token limit\n    max_input_length = 1024  # Ensure this is within the model's max token limit\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n    \n    # Set max_new_tokens to control the length of the generated answer\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=150, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Remove the prompt part from the output and return the answer\n    answer = generated_text[len(input_text):].strip()\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.158328Z","iopub.execute_input":"2025-01-10T18:49:19.158685Z","iopub.status.idle":"2025-01-10T18:49:19.196840Z","shell.execute_reply.started":"2025-01-10T18:49:19.158647Z","shell.execute_reply":"2025-01-10T18:49:19.195659Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n\n\ndef generate_answer_with_context_bert(query, context):\n    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n    model = BertForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n\n    # Prepare the input text for BERT\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Print the tokenized input to check if the context and question are properly encoded\n    # print(\"Tokenized Input:\", inputs)\n\n    # Run the model to get start and end logits\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n\n    # Print the logits to see what the model is predicting\n    # print(\"Start Scores:\", start_scores)\n    # print(\"End Scores:\", end_scores)\n\n    # Get the most likely start and end positions\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n\n    # Check if the answer span is valid\n    if answer_start >= answer_end:\n        print(\"No valid answer found.\")\n        return \"No valid answer found.\"\n\n    # Decode the answer (removes [CLS], [SEP] tokens and gives you the answer span)\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end], skip_special_tokens=True)\n    \n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.198033Z","iopub.execute_input":"2025-01-10T18:49:19.198321Z","iopub.status.idle":"2025-01-10T18:49:19.206198Z","shell.execute_reply.started":"2025-01-10T18:49:19.198296Z","shell.execute_reply":"2025-01-10T18:49:19.204583Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\nimport torch\n\ndef generate_answer_with_context_distilbert(query, context):\n    model_name = \"distilbert-base-uncased-distilled-squad\"\n    model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end])\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.207287Z","iopub.execute_input":"2025-01-10T18:49:19.207571Z","iopub.status.idle":"2025-01-10T18:49:19.240348Z","shell.execute_reply.started":"2025-01-10T18:49:19.207546Z","shell.execute_reply":"2025-01-10T18:49:19.238913Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import google.generativeai as genai\n\ndef generate_answer_with_context_gemini(query, context):\n    # Initialize Google Generative AI model\n    genai.configure(api_key=\"AIzaSyAMCnx5Wde22yIsRL53lB20FETVkCXe2Ws\")\n    # gemini_model = genai.GenerativeModel('gemini-pro')\n    gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n    \n    # Construct the prompt\n    prompt = f\"Given the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    try:\n        # Use the correct method to generate an answer, check if `generate_content` or `generate` is available\n        answer = gemini_model.generate_content(prompt)  # Correct method usage\n        # print(answer.text)\n\n        \n        # Print and return the answer\n        print(\"Generated Answer:\")\n        return answer.text\n        \n    except Exception as e:\n        print(f\"Error while generating content: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.244713Z","iopub.execute_input":"2025-01-10T18:49:19.245114Z","iopub.status.idle":"2025-01-10T18:49:19.252962Z","shell.execute_reply.started":"2025-01-10T18:49:19.245079Z","shell.execute_reply":"2025-01-10T18:49:19.251831Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import BloomForCausalLM, BloomTokenizerFast\n\ndef generate_answer_with_context_bloom(query, context):\n    model_name = \"bigscience/bloom-560m\"\n    model = BloomForCausalLM.from_pretrained(model_name)\n    tokenizer = BloomTokenizerFast.from_pretrained(model_name)\n\n    # Create the prompt\n    input_text = f\"Given the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    # Tokenize input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n\n    # Generate the response\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=100)  # Control only new token generation\n\n    # Decode the generated tokens to text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = generated_text.split('Question: ')[-1]\n\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.254632Z","iopub.execute_input":"2025-01-10T18:49:19.255047Z","iopub.status.idle":"2025-01-10T18:49:19.283229Z","shell.execute_reply.started":"2025-01-10T18:49:19.255013Z","shell.execute_reply":"2025-01-10T18:49:19.282136Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_answer_with_context_distilgpt2(query, context):\n    model_name = \"distilgpt2\"\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=150, temperature=0.9)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = generated_text.split('Question: ')[-1]\n\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.283992Z","iopub.execute_input":"2025-01-10T18:49:19.284253Z","iopub.status.idle":"2025-01-10T18:49:19.296645Z","shell.execute_reply.started":"2025-01-10T18:49:19.284230Z","shell.execute_reply":"2025-01-10T18:49:19.295546Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import ElectraForQuestionAnswering, ElectraTokenizer\nimport torch\n\ndef generate_answer_with_context_electra(query, context):\n    model_name = \"google/electra-small-discriminator\"\n    model = ElectraForQuestionAnswering.from_pretrained(model_name)\n    tokenizer = ElectraTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    outputs = model(**inputs)\n    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores) + 1\n    answer = tokenizer.decode(inputs.input_ids[0][answer_start:answer_end])\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.297894Z","iopub.execute_input":"2025-01-10T18:49:19.298280Z","iopub.status.idle":"2025-01-10T18:49:19.328213Z","shell.execute_reply.started":"2025-01-10T18:49:19.298248Z","shell.execute_reply":"2025-01-10T18:49:19.326961Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import ReformerForSequenceClassification, ReformerTokenizer\n\ndef generate_answer_with_context_reformer(query, context):\n    model_name = \"google/reformer-enwik8\"\n    model = ReformerForSequenceClassification.from_pretrained(model_name)\n    tokenizer = ReformerTokenizer.from_pretrained(model_name)\n\n    input_text = f\"Context: {context}\\nQuestion: {query}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n    outputs = model.generate(input_ids=inputs['input_ids'], max_length=150)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.329413Z","iopub.execute_input":"2025-01-10T18:49:19.329939Z","iopub.status.idle":"2025-01-10T18:49:19.349121Z","shell.execute_reply.started":"2025-01-10T18:49:19.329893Z","shell.execute_reply":"2025-01-10T18:49:19.347869Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# import torch\n# from transformers import pipeline\n\n# model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n# pipe = pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"auto\",\n# )\n# messages = [\n#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n# ]\n# outputs = pipe(\n#     messages,\n#     max_new_tokens=256,\n# )\n# print(outputs[0][\"generated_text\"][-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.350331Z","iopub.execute_input":"2025-01-10T18:49:19.350774Z","iopub.status.idle":"2025-01-10T18:49:19.356595Z","shell.execute_reply.started":"2025-01-10T18:49:19.350723Z","shell.execute_reply":"2025-01-10T18:49:19.355137Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# model_name_long = \"meta-llama/Llama-3.2-1B-Instruct\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name_long)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# log.info(f\"Loading the model {model_name_long}\")\n# bf16 = False\n# fp16 = True\n# if torch.cuda.is_available():\n#     major, _ = torch.cuda.get_device_capability()\n#     if major >= 8:\n#         log.info(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n#         bf16 = True\n#         fp16 = False\n# # Load the model\n# device_map = {\"\": 0}  # Load on GPU 0\n# torch_dtype = torch.bfloat16 if bf16 else torch.float16\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_name_long,\n#     torch_dtype=torch_dtype,\n#     device_map=device_map,\n# )\n# log.info(f\"Model loaded with torch_dtype={torch_dtype}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.357867Z","iopub.execute_input":"2025-01-10T18:49:19.358256Z","iopub.status.idle":"2025-01-10T18:49:19.381438Z","shell.execute_reply.started":"2025-01-10T18:49:19.358223Z","shell.execute_reply":"2025-01-10T18:49:19.380224Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\n# Load the FAISS index\nindex = faiss.read_index(\"faiss_index_final.bin\")\n\n# Example query\nquery = \"what is Bismuth Stibium ?\"\nquery_embedding = model.encode(query).astype('float32').reshape(1, -1)\n\n# Retrieve the top 5 most relevant contexts\ndistances, indices = index.search(query_embedding, 5)\n\n# Extract the relevant contexts\nrelevant_contexts = processed_df.iloc[indices[0]]['chunk'].tolist()\n\n# print('relevent context : ',relevant_contexts)\ncontext = relevant_contexts[0]\n\n# For GPT-2\ngenerated_answer = generate_answer_with_context_gpt2(query, context)\nprint(\"GPT-2 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For T5\ngenerated_answer = generate_answer_with_context_t5(query, context)\nprint(\"T5 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For BLOOM\ngenerated_answer = generate_answer_with_context_bloom(query, context)\nprint(\"bloom model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n\n# For GEMINI\ngenerated_answer = generate_answer_with_context_gemini(query, context)\nprint(\"GEMINI model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For GEMINI\n# generated_answer = generate_answer_with_context_reformer(query, context)\n# print(\"REFORMER model\")\n# print(generated_answer)\n# print(\"------------------------\")\n\n# For Electra\ngenerated_answer = generate_answer_with_context_electra(query, context)\nprint(\"Electra model\")\nprint(generated_answer)\nprint(\"------------------------\")\n\n# For Distilgpt2\ngenerated_answer = generate_answer_with_context_distilgpt2(query, context)\nprint(\"Distilgpt2 model\")\nprint(generated_answer)\nprint(\"------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:49:19.382369Z","iopub.execute_input":"2025-01-10T18:49:19.382780Z","iopub.status.idle":"2025-01-10T18:51:50.888019Z","shell.execute_reply.started":"2025-01-10T18:49:19.382733Z","shell.execute_reply":"2025-01-10T18:51:50.886539Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b206027f80614705a721ae5f333a17c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11c97514292f4934bdaa1f0451a49615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd81ba1c8284887b19967c8578ef843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d25ba612f3941a7a25449e3805eb270"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d9412650e649348a25b03cf07f698b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2658d6957ab42468a16bef118ba3117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee1c5cf6eac43f3bb0cc6e27da08a48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087530721bdd435c8694927aac2d40bf"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"GPT-2 model\nIt is a natural compound that is used to treat wart, ulcers, and other conditions. It has been used for centuries to relieve warty, sore throats, to prevent the spread of diseases, as well as to cure the common cold.\nBismutum St.ibum is an alkaloid found in the bark of the tree of B. stibus. The bark is composed of a mixture of two parts: the alkaline and the insoluble part. Bistum st.bismum contains a compound called bismulose, which is found naturally in bark. This compound is known as bistulosulfate. In the United States, it is also known by the name bicarbonate\n------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d329e5f666654929ba3d2853684390d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e92735991ca4040be889faeceba7ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eacaab3cfa24534a652fc2deb98a088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c48130e7f849b1b25ce29be4cd1562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740f3770fff14d7496c44afef671a4f9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"T5 model\nGARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJ\n------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b834ee44c94934ad5d27982d42ecf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e8c41adc4542d7909014183d39fb8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38750be7f341450184724fd8302e5a93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6892c35e021422881641351c75713e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed3481cc53e4b26863bc9a6c6370900"}},"metadata":{}},{"name":"stdout","text":"bloom model\nwhat is Bismuth Stibium ?\nAnswer: Bismuth Stibium is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived\n------------------------\nGenerated Answer:\nGEMINI model\nBismuth Stibium is a homeopathic ointment marketed for the temporary relief of warts.  It contains a mixture of homeopathic ingredients, including Bismuth, Stibium (Antimony), Garlic, Greater Celandine, Turmeric, and American Arborvitae, in a base of white petrolatum, lanolin, mineral oil, and other inactive ingredients.  It is manufactured by Uriel Pharmacy Inc.\n\n------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9271c65caf34675bf3bc101357cbc05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367ce9365c38462bb5fbc8991063a9ea"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66554f8becc240c3b3da94b2d9901d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7078564965994b2a82b3326cb6075c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b581bb7fb5c47239eadca74c2be4819"}},"metadata":{}},{"name":"stdout","text":"Electra model\n\n------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b529b56cad453189a653979b85628e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0d85de45254677b009e63db696e560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82fdf8272c8494db48d3c835d77582c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289ee6640d0c4aba9121348f72c0b589"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36556bdc87d54a1b84375a74b2957ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60886e40c2ef461982ea7fab975c7aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa06225a8f3b4331a36dbe647d5a3948"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Distilgpt2 model\nwhat is Bismuth Stibium?\nAnswer: Bismuth Stibium is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is a compound of the Bismuth Stibium, which is\n------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport time\nimport torch\nimport pandas as pd\nimport nltk\nnltk.download('punkt')  # Required for BLEU score\n\n# First, let's define our test dataset with some sample questions and reference answers\ntest_data = [\n    {\n        'question': 'What is Bismuth Stibium?',\n        'context': context,  # Your existing context\n        'reference_answer': 'Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.'\n    },\n    # Add more test cases if you have them\n]\n\n# Keep your existing model functions\nmodels = {\n    'GPT-2': generate_answer_with_context_gpt2,\n    'T5': generate_answer_with_context_t5,\n    # 'BERT': generate_answer_with_context_bert,\n    # 'DistilBERT': generate_answer_with_context_distilbert,\n    'BLOOM': generate_answer_with_context_bloom,\n     'GEMINI': generate_answer_with_context_gemini,\n    'DISTILGPT2': generate_answer_with_context_distilgpt2\n}\n\ndef evaluate_single_answer(predicted, reference):\n    # Initialize ROUGE scorer\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    \n    # ROUGE scores\n    rouge_scores = scorer.score(predicted, reference)\n    \n    # BLEU score\n    ref_tokens = [reference.split()]\n    pred_tokens = predicted.split()\n    try:\n        bleu = sentence_bleu(ref_tokens, pred_tokens)\n    except:\n        bleu = 0\n    \n    # Exact Match\n    exact_match = 1 if predicted.lower().strip() == reference.lower().strip() else 0\n    \n    # F1 Score\n    pred_words = set(predicted.lower().split())\n    ref_words = set(reference.lower().split())\n    \n    precision = len(pred_words & ref_words) / len(pred_words) if pred_words else 0\n    recall = len(pred_words & ref_words) / len(ref_words) if ref_words else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n    \n    return {\n        'rouge1': rouge_scores['rouge1'].fmeasure,\n        'rouge2': rouge_scores['rouge2'].fmeasure,\n        'rougeL': rouge_scores['rougeL'].fmeasure,\n        'bleu': bleu,\n        'exact_match': exact_match,\n        'f1': f1\n    }\n\ndef evaluate_model(model_name, model_func, test_data):\n    all_metrics = []\n    execution_times = []\n    \n    for test_case in test_data:\n        # Measure execution time\n        start_time = time.time()\n        predicted_answer = model_func(test_case['question'], test_case['context'])\n        execution_time = time.time() - start_time\n        \n        # Calculate metrics\n        metrics = evaluate_single_answer(predicted_answer, test_case['reference_answer'])\n        metrics['execution_time'] = execution_time\n        all_metrics.append(metrics)\n        \n        # Print individual results\n        print(f\"\\n{model_name} Results for Question: {test_case['question']}\")\n        print(f\"Predicted Answer: {predicted_answer}\")\n        print(f\"Reference Answer: {test_case['reference_answer']}\")\n        print(f\"Metrics: {metrics}\")\n    \n    # Calculate averages\n    avg_metrics = {\n        metric: np.mean([m[metric] for m in all_metrics])\n        for metric in all_metrics[0].keys()\n    }\n    \n    return avg_metrics\n\n# Run evaluation for all models\nresults = {}\nfor model_name, model_func in models.items():\n    print(f\"\\nEvaluating {model_name}...\")\n    try:\n        results[model_name] = evaluate_model(model_name, model_func, test_data)\n    except Exception as e:\n        print(f\"Error evaluating {model_name}: {str(e)}\")\n        continue\n\n# Create final report\ndf_results = pd.DataFrame(results).round(4)\nprint(\"\\nFinal Evaluation Results:\")\nprint(df_results)\n\n# Save results to CSV\ndf_results.to_csv('rag_evaluation_results.csv')\nprint(\"\\nResults saved to 'rag_evaluation_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T18:54:53.964462Z","iopub.execute_input":"2025-01-10T18:54:53.964938Z","execution_failed":"2025-01-10T22:23:03.567Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\nEvaluating GPT-2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\nGPT-2 Results for Question: What is Bismuth Stibium?\nPredicted Answer: This product is a topical product that is used to treat wart, ulcers, and other skin conditions. It is also used for topical use to prevent or treat other conditions such as acne, psoriasis, eczema, rashes, dry skin, skin cancer, etc.\nWhat is the difference between Bistuth and Bactulose? BISTUTH is an antiseptic that has been used in the treatment of psoriatic ulcerative colitis, acne vulgaris, dermatitis and psorsitis. BACTULOSE is another topical antisera that was used as a treatment for psorectal ulicosis. The difference is that BIS is not a preservative. This is because B\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.28723404255319157, 'rouge2': 0.053763440860215055, 'rougeL': 0.1595744680851064, 'bleu': 0.32030896835479866, 'exact_match': 0, 'f1': 0.20437956204379565, 'execution_time': 9.40396785736084}\n\nEvaluating T5...\n\nT5 Results for Question: What is Bismuth Stibium?\nPredicted Answer: GARLIC CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJUS CHELIDONIUM MAJ\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.03418803418803419, 'rouge2': 0.017391304347826087, 'rougeL': 0.03418803418803419, 'bleu': 0, 'exact_match': 0, 'f1': 0.025641025641025644, 'execution_time': 45.67960476875305}\n\nEvaluating BLOOM...\n\nBLOOM Results for Question: What is Bismuth Stibium?\nPredicted Answer: What is Bismuth Stibium?\nAnswer: Bismuth Stibium is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived from the roots of the herb Bismuth. It is a natural product derived\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.24175824175824176, 'rouge2': 0.044444444444444446, 'rougeL': 0.2087912087912088, 'bleu': 0.035371965931008, 'exact_match': 0, 'f1': 0.17582417582417584, 'execution_time': 26.763834476470947}\n\nEvaluating GEMINI...\nGenerated Answer:\n\nGEMINI Results for Question: What is Bismuth Stibium?\nPredicted Answer: Bismuth Stibium is a homeopathic topical ointment intended for the temporary relief of warts.  It contains a mixture of active ingredients including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium met. (Antimony), in a base of white petrolatum, lanolin, mineral oil, glyceryl monostearate, sorbic acid, tea tree oil, and grapefruit seed extract.\n\nReference Answer: Bismuth Stibium is a human over-the-counter (OTC) drug manufactured by Uriel Pharmacy Inc., used topically for the temporary relief of warts. It contains a blend of active ingredients, including Allium sativa (Garlic), Chelidonium (Greater celandine), Curcuma (Turmeric), Thuja (American arborvitae), Bismuth, and Stibium (Antimony). The product is for external use only and is marketed as a homeopathic remedy based on traditional practices, though it is not FDA evaluated. The ointment is applied once or twice daily to warts, with special instructions to consult a doctor for children under age 2.\nMetrics: {'rouge1': 0.5066666666666666, 'rouge2': 0.3918918918918919, 'rougeL': 0.4666666666666667, 'bleu': 0.2537643443585953, 'exact_match': 0, 'f1': 0.464, 'execution_time': 2.675204038619995}\n\nEvaluating DISTILGPT2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}